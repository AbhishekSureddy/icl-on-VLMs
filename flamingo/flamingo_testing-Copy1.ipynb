{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42f52e9-5404-467b-8c76-70b1f1e3b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  5 19:07:57 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:5F:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             68W /  500W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a31c9d-3677-4d1f-9ae2-072ea0a03a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asureddy_umass_edu/.conda/envs/open-flamingo/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b:\n",
      "- configuration_mosaic_gpt.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b:\n",
      "- mosaic_gpt.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/home/asureddy_umass_edu/.conda/envs/open-flamingo/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/asureddy_umass_edu/.conda/envs/open-flamingo/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/asureddy_umass_edu/.conda/envs/open-flamingo/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/scratch/workspace/asureddy_umass_edu-llm_alignment/hf-cache/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b/50d6bc94e17812873f39c36c5f815263fa71fb80/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "Flamingo model initialized with 1046992944 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    cross_attn_every_n_layers=1,\n",
    "    # cache_dir=\"/scratch/workspace/asureddy_umass_edu-llm_alignment/hf-cache\"  # Defaults to ~/.cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906d3141-114a-4020-ba3a-4c31edc44445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['vision_encoder.class_embedding', 'vision_encoder.positional_embedding', 'vision_encoder.proj', 'vision_encoder.conv1.weight', 'vision_encoder.ln_pre.weight', 'vision_encoder.ln_pre.bias', 'vision_encoder.transformer.resblocks.0.ln_1.weight', 'vision_encoder.transformer.resblocks.0.ln_1.bias', 'vision_encoder.transformer.resblocks.0.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.0.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.0.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.0.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.0.ln_2.weight', 'vision_encoder.transformer.resblocks.0.ln_2.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_1.weight', 'vision_encoder.transformer.resblocks.1.ln_1.bias', 'vision_encoder.transformer.resblocks.1.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.1.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.1.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.1.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_2.weight', 'vision_encoder.transformer.resblocks.1.ln_2.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_1.weight', 'vision_encoder.transformer.resblocks.2.ln_1.bias', 'vision_encoder.transformer.resblocks.2.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.2.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.2.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.2.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_2.weight', 'vision_encoder.transformer.resblocks.2.ln_2.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_1.weight', 'vision_encoder.transformer.resblocks.3.ln_1.bias', 'vision_encoder.transformer.resblocks.3.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.3.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.3.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.3.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_2.weight', 'vision_encoder.transformer.resblocks.3.ln_2.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_1.weight', 'vision_encoder.transformer.resblocks.4.ln_1.bias', 'vision_encoder.transformer.resblocks.4.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.4.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.4.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.4.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_2.weight', 'vision_encoder.transformer.resblocks.4.ln_2.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_1.weight', 'vision_encoder.transformer.resblocks.5.ln_1.bias', 'vision_encoder.transformer.resblocks.5.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.5.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.5.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.5.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_2.weight', 'vision_encoder.transformer.resblocks.5.ln_2.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_1.weight', 'vision_encoder.transformer.resblocks.6.ln_1.bias', 'vision_encoder.transformer.resblocks.6.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.6.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.6.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.6.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_2.weight', 'vision_encoder.transformer.resblocks.6.ln_2.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_1.weight', 'vision_encoder.transformer.resblocks.7.ln_1.bias', 'vision_encoder.transformer.resblocks.7.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.7.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.7.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.7.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_2.weight', 'vision_encoder.transformer.resblocks.7.ln_2.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_1.weight', 'vision_encoder.transformer.resblocks.8.ln_1.bias', 'vision_encoder.transformer.resblocks.8.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.8.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.8.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.8.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_2.weight', 'vision_encoder.transformer.resblocks.8.ln_2.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_1.weight', 'vision_encoder.transformer.resblocks.9.ln_1.bias', 'vision_encoder.transformer.resblocks.9.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.9.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.9.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.9.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_2.weight', 'vision_encoder.transformer.resblocks.9.ln_2.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_1.weight', 'vision_encoder.transformer.resblocks.10.ln_1.bias', 'vision_encoder.transformer.resblocks.10.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.10.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.10.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.10.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_2.weight', 'vision_encoder.transformer.resblocks.10.ln_2.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_1.weight', 'vision_encoder.transformer.resblocks.11.ln_1.bias', 'vision_encoder.transformer.resblocks.11.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.11.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.11.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.11.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_2.weight', 'vision_encoder.transformer.resblocks.11.ln_2.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_1.weight', 'vision_encoder.transformer.resblocks.12.ln_1.bias', 'vision_encoder.transformer.resblocks.12.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.12.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.12.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.12.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_2.weight', 'vision_encoder.transformer.resblocks.12.ln_2.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_1.weight', 'vision_encoder.transformer.resblocks.13.ln_1.bias', 'vision_encoder.transformer.resblocks.13.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.13.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.13.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.13.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_2.weight', 'vision_encoder.transformer.resblocks.13.ln_2.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_1.weight', 'vision_encoder.transformer.resblocks.14.ln_1.bias', 'vision_encoder.transformer.resblocks.14.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.14.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.14.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.14.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_2.weight', 'vision_encoder.transformer.resblocks.14.ln_2.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_1.weight', 'vision_encoder.transformer.resblocks.15.ln_1.bias', 'vision_encoder.transformer.resblocks.15.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.15.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.15.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.15.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_2.weight', 'vision_encoder.transformer.resblocks.15.ln_2.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_1.weight', 'vision_encoder.transformer.resblocks.16.ln_1.bias', 'vision_encoder.transformer.resblocks.16.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.16.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.16.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.16.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_2.weight', 'vision_encoder.transformer.resblocks.16.ln_2.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_1.weight', 'vision_encoder.transformer.resblocks.17.ln_1.bias', 'vision_encoder.transformer.resblocks.17.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.17.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.17.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.17.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_2.weight', 'vision_encoder.transformer.resblocks.17.ln_2.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_1.weight', 'vision_encoder.transformer.resblocks.18.ln_1.bias', 'vision_encoder.transformer.resblocks.18.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.18.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.18.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.18.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_2.weight', 'vision_encoder.transformer.resblocks.18.ln_2.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_1.weight', 'vision_encoder.transformer.resblocks.19.ln_1.bias', 'vision_encoder.transformer.resblocks.19.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.19.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.19.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.19.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_2.weight', 'vision_encoder.transformer.resblocks.19.ln_2.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_1.weight', 'vision_encoder.transformer.resblocks.20.ln_1.bias', 'vision_encoder.transformer.resblocks.20.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.20.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.20.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.20.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_2.weight', 'vision_encoder.transformer.resblocks.20.ln_2.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_1.weight', 'vision_encoder.transformer.resblocks.21.ln_1.bias', 'vision_encoder.transformer.resblocks.21.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.21.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.21.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.21.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_2.weight', 'vision_encoder.transformer.resblocks.21.ln_2.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_1.weight', 'vision_encoder.transformer.resblocks.22.ln_1.bias', 'vision_encoder.transformer.resblocks.22.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.22.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.22.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.22.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_2.weight', 'vision_encoder.transformer.resblocks.22.ln_2.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_1.weight', 'vision_encoder.transformer.resblocks.23.ln_1.bias', 'vision_encoder.transformer.resblocks.23.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.23.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.23.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.23.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_2.weight', 'vision_encoder.transformer.resblocks.23.ln_2.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.bias', 'vision_encoder.ln_post.weight', 'vision_encoder.ln_post.bias', 'lang_encoder.transformer.blocks.0.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.ln_f.weight', 'lang_encoder.old_decoder_blocks.0.ln_1.weight', 'lang_encoder.old_decoder_blocks.0.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.0.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.0.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.0.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.0.ln_2.weight', 'lang_encoder.old_decoder_blocks.0.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.0.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.1.ln_1.weight', 'lang_encoder.old_decoder_blocks.1.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.1.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.1.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.1.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.1.ln_2.weight', 'lang_encoder.old_decoder_blocks.1.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.1.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.2.ln_1.weight', 'lang_encoder.old_decoder_blocks.2.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.2.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.2.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.2.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.2.ln_2.weight', 'lang_encoder.old_decoder_blocks.2.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.2.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.3.ln_1.weight', 'lang_encoder.old_decoder_blocks.3.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.3.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.3.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.3.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.3.ln_2.weight', 'lang_encoder.old_decoder_blocks.3.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.3.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.4.ln_1.weight', 'lang_encoder.old_decoder_blocks.4.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.4.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.4.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.4.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.4.ln_2.weight', 'lang_encoder.old_decoder_blocks.4.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.4.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.5.ln_1.weight', 'lang_encoder.old_decoder_blocks.5.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.5.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.5.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.5.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.5.ln_2.weight', 'lang_encoder.old_decoder_blocks.5.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.5.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.6.ln_1.weight', 'lang_encoder.old_decoder_blocks.6.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.6.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.6.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.6.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.6.ln_2.weight', 'lang_encoder.old_decoder_blocks.6.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.6.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.7.ln_1.weight', 'lang_encoder.old_decoder_blocks.7.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.7.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.7.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.7.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.7.ln_2.weight', 'lang_encoder.old_decoder_blocks.7.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.7.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.8.ln_1.weight', 'lang_encoder.old_decoder_blocks.8.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.8.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.8.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.8.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.8.ln_2.weight', 'lang_encoder.old_decoder_blocks.8.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.8.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.9.ln_1.weight', 'lang_encoder.old_decoder_blocks.9.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.9.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.9.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.9.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.9.ln_2.weight', 'lang_encoder.old_decoder_blocks.9.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.9.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.10.ln_1.weight', 'lang_encoder.old_decoder_blocks.10.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.10.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.10.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.10.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.10.ln_2.weight', 'lang_encoder.old_decoder_blocks.10.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.10.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.11.ln_1.weight', 'lang_encoder.old_decoder_blocks.11.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.11.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.11.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.11.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.11.ln_2.weight', 'lang_encoder.old_decoder_blocks.11.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.11.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.12.ln_1.weight', 'lang_encoder.old_decoder_blocks.12.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.12.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.12.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.12.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.12.ln_2.weight', 'lang_encoder.old_decoder_blocks.12.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.12.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.13.ln_1.weight', 'lang_encoder.old_decoder_blocks.13.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.13.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.13.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.13.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.13.ln_2.weight', 'lang_encoder.old_decoder_blocks.13.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.13.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.14.ln_1.weight', 'lang_encoder.old_decoder_blocks.14.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.14.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.14.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.14.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.14.ln_2.weight', 'lang_encoder.old_decoder_blocks.14.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.14.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.15.ln_1.weight', 'lang_encoder.old_decoder_blocks.15.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.15.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.15.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.15.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.15.ln_2.weight', 'lang_encoder.old_decoder_blocks.15.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.15.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.16.ln_1.weight', 'lang_encoder.old_decoder_blocks.16.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.16.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.16.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.16.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.16.ln_2.weight', 'lang_encoder.old_decoder_blocks.16.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.16.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.17.ln_1.weight', 'lang_encoder.old_decoder_blocks.17.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.17.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.17.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.17.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.17.ln_2.weight', 'lang_encoder.old_decoder_blocks.17.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.17.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.18.ln_1.weight', 'lang_encoder.old_decoder_blocks.18.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.18.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.18.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.18.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.18.ln_2.weight', 'lang_encoder.old_decoder_blocks.18.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.18.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.19.ln_1.weight', 'lang_encoder.old_decoder_blocks.19.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.19.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.19.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.19.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.19.ln_2.weight', 'lang_encoder.old_decoder_blocks.19.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.19.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.20.ln_1.weight', 'lang_encoder.old_decoder_blocks.20.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.20.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.20.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.20.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.20.ln_2.weight', 'lang_encoder.old_decoder_blocks.20.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.20.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.21.ln_1.weight', 'lang_encoder.old_decoder_blocks.21.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.21.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.21.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.21.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.21.ln_2.weight', 'lang_encoder.old_decoder_blocks.21.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.21.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.22.ln_1.weight', 'lang_encoder.old_decoder_blocks.22.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.22.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.22.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.22.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.22.ln_2.weight', 'lang_encoder.old_decoder_blocks.22.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.22.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.23.ln_1.weight', 'lang_encoder.old_decoder_blocks.23.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.23.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.23.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.23.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.23.ln_2.weight', 'lang_encoder.old_decoder_blocks.23.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.23.mlp.mlp_down.weight', 'lang_encoder.gated_cross_attn_layers.0.attn_gate', 'lang_encoder.gated_cross_attn_layers.0.ff_gate', 'lang_encoder.gated_cross_attn_layers.0.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.0.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.0.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.0.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.0.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.0.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.0.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.0.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.0.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.1.attn_gate', 'lang_encoder.gated_cross_attn_layers.1.ff_gate', 'lang_encoder.gated_cross_attn_layers.1.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.1.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.1.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.2.attn_gate', 'lang_encoder.gated_cross_attn_layers.2.ff_gate', 'lang_encoder.gated_cross_attn_layers.2.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.2.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.2.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.2.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.2.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.2.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.2.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.2.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.2.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.3.attn_gate', 'lang_encoder.gated_cross_attn_layers.3.ff_gate', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.3.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.3.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.4.attn_gate', 'lang_encoder.gated_cross_attn_layers.4.ff_gate', 'lang_encoder.gated_cross_attn_layers.4.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.4.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.4.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.4.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.4.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.4.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.4.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.4.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.4.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.5.attn_gate', 'lang_encoder.gated_cross_attn_layers.5.ff_gate', 'lang_encoder.gated_cross_attn_layers.5.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.5.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.5.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.6.attn_gate', 'lang_encoder.gated_cross_attn_layers.6.ff_gate', 'lang_encoder.gated_cross_attn_layers.6.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.6.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.6.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.6.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.6.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.6.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.6.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.6.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.6.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.7.attn_gate', 'lang_encoder.gated_cross_attn_layers.7.ff_gate', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.7.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.7.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.8.attn_gate', 'lang_encoder.gated_cross_attn_layers.8.ff_gate', 'lang_encoder.gated_cross_attn_layers.8.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.8.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.8.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.8.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.8.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.8.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.8.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.8.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.8.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.9.attn_gate', 'lang_encoder.gated_cross_attn_layers.9.ff_gate', 'lang_encoder.gated_cross_attn_layers.9.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.9.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.9.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.10.attn_gate', 'lang_encoder.gated_cross_attn_layers.10.ff_gate', 'lang_encoder.gated_cross_attn_layers.10.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.10.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.10.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.10.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.10.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.10.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.10.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.10.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.10.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.11.attn_gate', 'lang_encoder.gated_cross_attn_layers.11.ff_gate', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.11.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.11.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.12.attn_gate', 'lang_encoder.gated_cross_attn_layers.12.ff_gate', 'lang_encoder.gated_cross_attn_layers.12.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.12.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.12.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.12.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.12.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.12.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.12.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.12.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.12.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.13.attn_gate', 'lang_encoder.gated_cross_attn_layers.13.ff_gate', 'lang_encoder.gated_cross_attn_layers.13.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.13.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.13.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.14.attn_gate', 'lang_encoder.gated_cross_attn_layers.14.ff_gate', 'lang_encoder.gated_cross_attn_layers.14.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.14.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.14.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.14.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.14.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.14.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.14.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.14.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.14.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.15.attn_gate', 'lang_encoder.gated_cross_attn_layers.15.ff_gate', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.15.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.15.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.16.attn_gate', 'lang_encoder.gated_cross_attn_layers.16.ff_gate', 'lang_encoder.gated_cross_attn_layers.16.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.16.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.16.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.16.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.16.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.16.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.16.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.16.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.16.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.17.attn_gate', 'lang_encoder.gated_cross_attn_layers.17.ff_gate', 'lang_encoder.gated_cross_attn_layers.17.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.17.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.17.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.18.attn_gate', 'lang_encoder.gated_cross_attn_layers.18.ff_gate', 'lang_encoder.gated_cross_attn_layers.18.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.18.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.18.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.18.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.18.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.18.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.18.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.18.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.18.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.19.attn_gate', 'lang_encoder.gated_cross_attn_layers.19.ff_gate', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.19.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.19.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.20.attn_gate', 'lang_encoder.gated_cross_attn_layers.20.ff_gate', 'lang_encoder.gated_cross_attn_layers.20.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.20.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.20.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.20.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.20.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.20.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.20.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.20.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.20.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.21.attn_gate', 'lang_encoder.gated_cross_attn_layers.21.ff_gate', 'lang_encoder.gated_cross_attn_layers.21.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.21.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.21.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.22.attn_gate', 'lang_encoder.gated_cross_attn_layers.22.ff_gate', 'lang_encoder.gated_cross_attn_layers.22.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.22.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.22.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.22.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.22.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.22.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.22.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.22.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.22.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.23.attn_gate', 'lang_encoder.gated_cross_attn_layers.23.ff_gate', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.23.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.23.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.3.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9842f082-aad2-411b-94b3-ed8f9ca68020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9051b52-a47a-4649-8eb5-b43c9b5d19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53190564-fdd0-40a5-b293-4c6352b7e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5008c481-94a8-406c-9c3e-5503a40857c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/val2017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2e37b99-d48c-4fe5-8b5c-4f64852b9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = f\"{dataset_dir}/000000039769.jpg\"\n",
    "demo_image_one = Image.open(im1)\n",
    "# \"There are two cats laying down with two remotes\"\n",
    "im2 = f\"{dataset_dir}/000000500826.jpg\"\n",
    "demo_image_two = Image.open(im2)\n",
    "# im2\n",
    "# \"A pole with three road signs stands in front of a building.\"\n",
    "im3 = f\"{dataset_dir}/000000181421.jpg\"\n",
    "query_image = Image.open(im3)\n",
    "# query_image\n",
    "# \"Three people in a boat with an umbrella in the rain \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db3e4763-cc0e-4d49-9d94-832d2a559d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"<image>There are two cats laying down with two remotes.<|endofchunk|><image>A pole with three road signs stands in front of a building..<|endofchunk|><image>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af799fc9-2827-4b49-8c24-bef33064027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>There are two cats laying down with two remotes.<|endofchunk|><image>A pole with three road signs stands in front of a building..<|endofchunk|><image>A woman in a yellow umbrella sits in a boat on a lake.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [query],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x.cuda(),\n",
    "    lang_x=lang_x[\"input_ids\"].cuda(),\n",
    "    attention_mask=lang_x[\"attention_mask\"].cuda(),\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2afa044-c809-4d86-9adc-6e8bc9dbee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:   a woman and two children in a boat on a lake.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text: \", tokenizer.decode(generated_text[0])[len(query):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae17154e-d9e4-4506-b653-07b1b37dec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>\n",
      "caption:two cats.<|endofchunk|><image>\n",
      "caption:bathroom sink.<|endofchunk|><image>\n",
      "caption:Thanksgiving dinner.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "# icl for captioning task\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>\\ncaption:two cats.<|endofchunk|><image>\\ncaption:bathroom sink.<|endofchunk|><image>\\ncaption:\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=1,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea09ee8d-28d2-4413-940e-608984fe1b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>\n",
      "caption:two cats.<|endofchunk|><image>\n",
      "caption:bathroom sink.<|endofchunk|><image>\n",
      "caption:Thanksgiving dinner.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "# icl for captioning task\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>\\ncaption:two cats.<|endofchunk|><image>\\ncaption:bathroom sink.<|endofchunk|><image>\\ncaption:\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x.to('cuda'),\n",
    "    lang_x=lang_x[\"input_ids\"].to('cuda'),\n",
    "    attention_mask=lang_x[\"attention_mask\"].to('cuda'),\n",
    "    max_new_tokens=20,\n",
    "    num_beams=1,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8330038-0620-4310-8759-be14eb9436d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
