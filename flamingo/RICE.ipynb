{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5412871e-ed9c-4ac7-9eb2-592ef20255a8",
   "metadata": {},
   "source": [
    "## One-time notebook to pre-compute features for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13abbf47-2caa-437b-b5fb-8f94f4fec613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# from utils import custom_collate_fn\n",
    "from eval_datasets import CaptionDataset\n",
    "from eval_datasets import VQADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74f491b-4e15-4381-abc1-a18a0d66b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader that collates a list of dicts into a dict of lists.\n",
    "    \"\"\"\n",
    "    collated_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        collated_batch[key] = [item[key] for item in batch]\n",
    "    return collated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bbc9be-9b39-43fb-8f22-6e7fbf0443e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RICES:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        device,\n",
    "        batch_size,\n",
    "        vision_encoder_path=\"ViT-B-32\",\n",
    "        vision_encoder_pretrained=\"openai\",\n",
    "        cached_features=None,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Load the model and processor\n",
    "        vision_encoder, _, image_processor = open_clip.create_model_and_transforms(\n",
    "            vision_encoder_path,\n",
    "            pretrained=vision_encoder_pretrained,\n",
    "        )\n",
    "        self.model = vision_encoder.to(self.device)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        # Precompute features\n",
    "        if cached_features is None:\n",
    "            self.features = self._precompute_features()\n",
    "        else:\n",
    "            self.features = cached_features\n",
    "\n",
    "    def _precompute_features(self):\n",
    "        features = []\n",
    "\n",
    "        # Switch to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Set up loader\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=custom_collate_fn,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(\n",
    "                loader,\n",
    "                desc=\"Precomputing features for RICES\",\n",
    "            ):\n",
    "                batch = batch[\"image\"]\n",
    "                inputs = torch.stack(\n",
    "                    [self.image_processor(image) for image in batch]\n",
    "                ).to(self.device)\n",
    "                image_features = self.model.encode_image(inputs)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                features.append(image_features.detach())\n",
    "\n",
    "        features = torch.cat(features)\n",
    "        return features\n",
    "\n",
    "    def find(self, batch, num_examples):\n",
    "        \"\"\"\n",
    "        Get the top num_examples most similar examples to the images.\n",
    "        \"\"\"\n",
    "        # Switch to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.stack([self.image_processor(image) for image in batch]).to(\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "            # Get the feature of the input image\n",
    "            query_feature = self.model.encode_image(inputs)\n",
    "            query_feature /= query_feature.norm(dim=-1, keepdim=True)\n",
    "            query_feature = query_feature.detach().cpu()\n",
    "\n",
    "            if query_feature.ndim == 1:\n",
    "                query_feature = query_feature.unsqueeze(0)\n",
    "\n",
    "            # Compute the similarity of the input image to the precomputed features\n",
    "            similarity = (query_feature @ self.features.T).squeeze()\n",
    "\n",
    "            if similarity.ndim == 1:\n",
    "                similarity = similarity.unsqueeze(0)\n",
    "\n",
    "            # Get the indices of the 'num_examples' most similar images\n",
    "            indices = similarity.argsort(dim=-1, descending=True)[:, :num_examples]\n",
    "\n",
    "        # Return with the most similar images last\n",
    "        return [[self.dataset[i] for i in reversed(row)] for row in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca662b-d641-4409-8f0e-dd185471cc76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Caption coco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fc301-076d-41fb-a62c-85476ac51c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = \"../dataset/annotations/captions_train2017.json\"\n",
    "image_dir_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/train2017\"\n",
    "dataset = CaptionDataset(image_dir_path, annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70443d6c-bbe6-4fab-ac4a-3c4980b7ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing features for RICES: 100%|██████████████████████████████████████████████| 370/370 [26:13<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever = RICES(dataset,\"cuda\",320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e6c0f3-91c0-4bee-8a9d-704f842ff7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0104,  0.0480,  0.0346,  ...,  0.0827,  0.0402,  0.0019],\n",
       "        [-0.0458, -0.0148,  0.0108,  ...,  0.0407, -0.0092, -0.0045],\n",
       "        [ 0.0483, -0.0166,  0.0134,  ...,  0.0371,  0.0147, -0.0147],\n",
       "        ...,\n",
       "        [-0.0217,  0.0206,  0.0063,  ...,  0.0627,  0.0450, -0.0022],\n",
       "        [-0.0073,  0.0270, -0.0074,  ...,  0.0908,  0.0426,  0.0249],\n",
       "        [-0.0177,  0.0144, -0.0181,  ...,  0.1143,  0.0487, -0.0009]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc6325e-88dd-4d35-894e-84128d8fd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving train-coco image features\n",
    "save_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/features-cache/coco_train.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bebb417d-5cbb-4e0a-8baa-ed242b4f79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(save_path,'wb') as f:\n",
    "    pickle.dump(retriever.features.cpu(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2942e87a-b0d4-4f6a-988c-134afd4aad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,'rb') as f:\n",
    "    ret_f2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99f4c65-c01b-4788-9530-7f75dba10546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0104,  0.0480,  0.0346,  ...,  0.0827,  0.0402,  0.0019],\n",
       "        [-0.0458, -0.0148,  0.0108,  ...,  0.0407, -0.0092, -0.0045],\n",
       "        [ 0.0483, -0.0166,  0.0134,  ...,  0.0371,  0.0147, -0.0147],\n",
       "        ...,\n",
       "        [-0.0217,  0.0206,  0.0063,  ...,  0.0627,  0.0450, -0.0022],\n",
       "        [-0.0073,  0.0270, -0.0074,  ...,  0.0908,  0.0426,  0.0249],\n",
       "        [-0.0177,  0.0144, -0.0181,  ...,  0.1143,  0.0487, -0.0009]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19304f15-2bf0-4e54-87a8-c9ce739579e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118287, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_f2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f838f22-e056-40d7-822f-cec7b871fafd",
   "metadata": {},
   "source": [
    "### VQA - coco 2014 train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f1f5fe-c7ab-4e67-ba2a-63bb73aea996",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/train2014\"\n",
    "questions_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "annotations_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/v2_mscoco_train2014_annotations.json\"\n",
    "dataset = VQADataset(image_dir_path, questions_path, annotations_path,True, \"vqav2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0222150f-f284-451c-8afc-6c58ecf26e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing features for RICES: 100%|█████████████████████████████████| 259/259 [15:20<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever = RICES(dataset,\"cuda\",320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76af28e6-babf-404c-8cf9-23a949c052e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3744e-02,  2.3473e-02,  2.2617e-02,  ..., -7.0341e-03,\n",
       "         -2.1294e-02, -4.3456e-02],\n",
       "        [-5.6258e-02,  3.1442e-02, -5.4619e-03,  ...,  1.9843e-02,\n",
       "          9.1135e-03,  3.2896e-02],\n",
       "        [ 1.2168e-02,  7.9854e-03, -1.5400e-02,  ...,  5.4900e-02,\n",
       "         -2.1708e-02, -2.3898e-02],\n",
       "        ...,\n",
       "        [-1.6284e-02,  2.7989e-03, -4.2672e-03,  ...,  9.8049e-02,\n",
       "          7.1288e-03, -2.5878e-05],\n",
       "        [-2.0299e-02,  2.4458e-02, -4.5172e-03,  ...,  8.3662e-02,\n",
       "         -1.6326e-02, -1.1608e-02],\n",
       "        [ 6.3887e-03,  5.9976e-02, -9.9650e-03,  ...,  7.5395e-02,\n",
       "         -9.3653e-03, -7.6931e-03]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7fe7b6-2d84-46ef-87de-2ac15c54d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving train-coco image features\n",
    "save_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/features-cache/coco_train_2014.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1705fb87-eaf4-4f1b-8b54-48b5473b93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(save_path,'wb') as f:\n",
    "    pickle.dump(retriever.features.cpu(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c645409-9ecd-4658-bab6-ecd8baf7e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,'rb') as f:\n",
    "    ret_f2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e4b019-c7dd-4cc9-a31d-d00fd1f9287e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3744e-02,  2.3473e-02,  2.2617e-02,  ..., -7.0341e-03,\n",
       "         -2.1294e-02, -4.3456e-02],\n",
       "        [-5.6258e-02,  3.1442e-02, -5.4619e-03,  ...,  1.9843e-02,\n",
       "          9.1135e-03,  3.2896e-02],\n",
       "        [ 1.2168e-02,  7.9854e-03, -1.5400e-02,  ...,  5.4900e-02,\n",
       "         -2.1708e-02, -2.3898e-02],\n",
       "        ...,\n",
       "        [-1.6284e-02,  2.7989e-03, -4.2672e-03,  ...,  9.8049e-02,\n",
       "          7.1288e-03, -2.5878e-05],\n",
       "        [-2.0299e-02,  2.4458e-02, -4.5172e-03,  ...,  8.3662e-02,\n",
       "         -1.6326e-02, -1.1608e-02],\n",
       "        [ 6.3887e-03,  5.9976e-02, -9.9650e-03,  ...,  7.5395e-02,\n",
       "         -9.3653e-03, -7.6931e-03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9856e4d9-3c24-4991-ad21-e4f73ac4f606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([82783, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_f2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
