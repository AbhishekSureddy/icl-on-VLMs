{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13abbf47-2caa-437b-b5fb-8f94f4fec613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asureddy_umass_edu/.conda/envs/open-flamingo/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# from utils import custom_collate_fn\n",
    "from eval_datasets import CaptionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17995381-9257-4a17-a98e-256978239fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = \"../dataset/annotations/captions_train2017.json\"\n",
    "image_dir_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/train2017\"\n",
    "dataset = CaptionDataset(image_dir_path, annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74f491b-4e15-4381-abc1-a18a0d66b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader that collates a list of dicts into a dict of lists.\n",
    "    \"\"\"\n",
    "    collated_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        collated_batch[key] = [item[key] for item in batch]\n",
    "    return collated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7293dd-713f-4cfb-8535-58436c891fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=10,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5bbc9be-9b39-43fb-8f22-6e7fbf0443e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RICES:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        device,\n",
    "        batch_size,\n",
    "        vision_encoder_path=\"ViT-B-32\",\n",
    "        vision_encoder_pretrained=\"openai\",\n",
    "        cached_features=None,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Load the model and processor\n",
    "        vision_encoder, _, image_processor = open_clip.create_model_and_transforms(\n",
    "            vision_encoder_path,\n",
    "            pretrained=vision_encoder_pretrained,\n",
    "        )\n",
    "        self.model = vision_encoder.to(self.device)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        # Precompute features\n",
    "        if cached_features is None:\n",
    "            self.features = self._precompute_features()\n",
    "        else:\n",
    "            self.features = cached_features\n",
    "\n",
    "    def _precompute_features(self):\n",
    "        features = []\n",
    "\n",
    "        # Switch to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Set up loader\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=custom_collate_fn,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(\n",
    "                loader,\n",
    "                desc=\"Precomputing features for RICES\",\n",
    "            ):\n",
    "                batch = batch[\"image\"]\n",
    "                inputs = torch.stack(\n",
    "                    [self.image_processor(image) for image in batch]\n",
    "                ).to(self.device)\n",
    "                image_features = self.model.encode_image(inputs)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                features.append(image_features.detach())\n",
    "\n",
    "        features = torch.cat(features)\n",
    "        return features\n",
    "\n",
    "    def find(self, batch, num_examples):\n",
    "        \"\"\"\n",
    "        Get the top num_examples most similar examples to the images.\n",
    "        \"\"\"\n",
    "        # Switch to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.stack([self.image_processor(image) for image in batch]).to(\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "            # Get the feature of the input image\n",
    "            query_feature = self.model.encode_image(inputs)\n",
    "            query_feature /= query_feature.norm(dim=-1, keepdim=True)\n",
    "            query_feature = query_feature.detach().cpu()\n",
    "\n",
    "            if query_feature.ndim == 1:\n",
    "                query_feature = query_feature.unsqueeze(0)\n",
    "\n",
    "            # Compute the similarity of the input image to the precomputed features\n",
    "            similarity = (query_feature @ self.features.T).squeeze()\n",
    "\n",
    "            if similarity.ndim == 1:\n",
    "                similarity = similarity.unsqueeze(0)\n",
    "\n",
    "            # Get the indices of the 'num_examples' most similar images\n",
    "            indices = similarity.argsort(dim=-1, descending=True)[:, :num_examples]\n",
    "\n",
    "        # Return with the most similar images last\n",
    "        return [[self.dataset[i] for i in reversed(row)] for row in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70443d6c-bbe6-4fab-ac4a-3c4980b7ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing features for RICES: 100%|██████████████████████████████████████████████| 370/370 [26:13<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever = RICES(dataset,\"cuda\",320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e6c0f3-91c0-4bee-8a9d-704f842ff7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0104,  0.0480,  0.0346,  ...,  0.0827,  0.0402,  0.0019],\n",
       "        [-0.0458, -0.0148,  0.0108,  ...,  0.0407, -0.0092, -0.0045],\n",
       "        [ 0.0483, -0.0166,  0.0134,  ...,  0.0371,  0.0147, -0.0147],\n",
       "        ...,\n",
       "        [-0.0217,  0.0206,  0.0063,  ...,  0.0627,  0.0450, -0.0022],\n",
       "        [-0.0073,  0.0270, -0.0074,  ...,  0.0908,  0.0426,  0.0249],\n",
       "        [-0.0177,  0.0144, -0.0181,  ...,  0.1143,  0.0487, -0.0009]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc6325e-88dd-4d35-894e-84128d8fd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving train-coco image features\n",
    "save_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/features-cache/coco_train.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bebb417d-5cbb-4e0a-8baa-ed242b4f79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(save_path,'wb') as f:\n",
    "    pickle.dump(retriever.features.cpu(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2942e87a-b0d4-4f6a-988c-134afd4aad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,'rb') as f:\n",
    "    ret_f2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99f4c65-c01b-4788-9530-7f75dba10546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0104,  0.0480,  0.0346,  ...,  0.0827,  0.0402,  0.0019],\n",
       "        [-0.0458, -0.0148,  0.0108,  ...,  0.0407, -0.0092, -0.0045],\n",
       "        [ 0.0483, -0.0166,  0.0134,  ...,  0.0371,  0.0147, -0.0147],\n",
       "        ...,\n",
       "        [-0.0217,  0.0206,  0.0063,  ...,  0.0627,  0.0450, -0.0022],\n",
       "        [-0.0073,  0.0270, -0.0074,  ...,  0.0908,  0.0426,  0.0249],\n",
       "        [-0.0177,  0.0144, -0.0181,  ...,  0.1143,  0.0487, -0.0009]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19304f15-2bf0-4e54-87a8-c9ce739579e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([118287, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_f2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca19555-c2e4-477f-9e1b-4bb1a112d8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
