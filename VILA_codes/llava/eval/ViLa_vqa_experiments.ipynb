{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1ce713-27bd-4da6-8698-c08c2f817666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-17 05:44:43,358] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/open_clip/factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129c2d94cd634cc2a5149004ddf5baa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcf32d3ef66431f9d386dca996eac3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "import re\n",
    "from io import BytesIO\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from llava.constants import (\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    ")\n",
    "from llava.conversation import SeparatorStyle, conv_templates\n",
    "from llava.mm_utils import KeywordsStoppingCriteria, get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from eval_datasets import VQADataset\n",
    "from rice import RICES\n",
    "\n",
    "\n",
    "# arg_parser = argparse.ArgumentParser()\n",
    "# arg_parser.add_argument(\n",
    "#     \"--model_name_or_path\",\n",
    "#     type=str,\n",
    "#     default= \"Efficient-Large-Model/VILA1.5-13b\",\n",
    "#     help=\"vila model name or path\"\n",
    "# )\n",
    "# arg_parser.add_argument(\n",
    "#     \"--n_shots\",\n",
    "#     type=int,\n",
    "#     help=\"number of incontext examples\",\n",
    "# )\n",
    "# arg_parser.add_argument(\n",
    "#     \"--use_random\",\n",
    "#     action=\"store_true\",\n",
    "#     help=\"Pass in a list of MMC4 shards in the format path_to_shard/shard_{0..23098}.zip\",\n",
    "# )\n",
    "# arg_parser.add_argument(\n",
    "#     \"--n_random\",\n",
    "#     type=int,\n",
    "#     default=0,\n",
    "#     help=\"number of random incontext examples\",\n",
    "# )\n",
    "# arg_parser.add_argument(\n",
    "#     \"--save_path\",\n",
    "#     type=str,\n",
    "#     help=\"where to save model outputs\",\n",
    "# )\n",
    "\n",
    "\n",
    "# args = arg_parser.parse_args()\n",
    "\n",
    "# input args\n",
    "model_name_or_path = \"Efficient-Large-Model/VILA1.5-13b\"\n",
    "\n",
    "train_image_dir_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/train2014\"\n",
    "train_questions_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "train_annotations_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/v2_mscoco_train2014_annotations.json\"\n",
    "\n",
    "val_image_dir_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/val2014\"\n",
    "val_questions_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "val_annotations_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/dataset/vqa/v2_mscoco_val2014_annotations.json\"\n",
    "\n",
    "# dataset = VQADataset(image_dir_path, questions_path, annotations_path,True, \"vqav2\")\n",
    "\n",
    "rice_cached_features_path = \"/scratch/workspace/asureddy_umass_edu-llm_alignment/features-cache/coco_train_2014.pkl\" \n",
    "train_dataset = VQADataset(train_image_dir_path, train_questions_path, train_annotations_path,True, \"vqav2\")\n",
    "val_dataset = VQADataset(val_image_dir_path, val_questions_path, val_annotations_path,False, \"vqav2\")\n",
    "if rice_cached_features_path:\n",
    "    with open(rice_cached_features_path, 'rb') as f:\n",
    "        rice_cached_features = pickle.load(f)\n",
    "\n",
    "retriever = RICES(train_dataset, 'cpu',1, cached_features=rice_cached_features)\n",
    "\n",
    "model_name = get_model_name_from_path(model_name_or_path)\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_name_or_path, model_name, None)\n",
    "image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "\n",
    "# conv_mode = \"hermes-2\" # for vila-40b\n",
    "# llava_v0 for vila-13b\n",
    "def get_output_for_query(query, imgs, conv_mode=\"llava_v1\",max_new_tokens=5):\n",
    "    query = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, query)\n",
    "    # conv_mode = \"hermes-2\"\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], query)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    \n",
    "    images_tensor = process_images(imgs, image_processor, model.config).to(model.device, dtype=torch.float16)\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    \n",
    "    # print(images_tensor.shape)\n",
    "    temperature = 0.2\n",
    "    num_beams = 3\n",
    "    top_p = 0.95\n",
    "    max_new_tokens = max_new_tokens\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=[\n",
    "                images_tensor,\n",
    "            ],\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "        )\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    outputs = outputs.strip()\n",
    "    # print(outputs)\n",
    "    if outputs.endswith(stop_str):\n",
    "        outputs = outputs[: -len(stop_str)]\n",
    "    outputs = outputs.strip()\n",
    "    # print(outputs)\n",
    "    return outputs\n",
    "\n",
    "def get_n_shot_demonstrations(item, n=2, use_random=True, n_random=0):\n",
    "    if n==0: return [[]]\n",
    "    if use_random:\n",
    "        train_idxs = list(np.random.choice(len(train_dataset),n))\n",
    "        icl_demonstrs = [[train_dataset[idx] for idx in train_idxs]]\n",
    "    else:\n",
    "        icl_demonstrs = []\n",
    "        if n_random:\n",
    "            train_idxs = list(np.random.choice(len(train_dataset),n_random))\n",
    "            icl_demonstrs = [train_dataset[idx] for idx in train_idxs]\n",
    "        icl_demonstrs = [icl_demonstrs + retriever.find([item['image']],n-n_random)[0]]\n",
    "        \n",
    "    return icl_demonstrs\n",
    "\n",
    "def construct_vqa_query(query_items, icl_demonstrs_list):\n",
    "    querys, im_lists = [], []\n",
    "    for query_item,icl_demonstrs in zip(query_items, icl_demonstrs_list):\n",
    "        # query = \"Answer the questions in one or two words: \"\n",
    "        query = \"\"\n",
    "        images = []\n",
    "        for item in icl_demonstrs:\n",
    "            query += f\" <image> Question: {item['question']} Short Answer: {item['answers'][0]}\"\n",
    "            images.append(item['image'])\n",
    "        images.append(query_item['image'])\n",
    "        query += f\"<image> Question: {query_item['question']} Short Answer: \"\n",
    "        querys.append(query)\n",
    "        im_lists.append(images)\n",
    "    return querys, im_lists\n",
    "\n",
    "def get_output(item, n= 2, use_random= False, n_random=0):\n",
    "\n",
    "    icl_demonstrs = get_n_shot_demonstrations(item, n, use_random, n_random)\n",
    "    querys, im_lists = construct_vqa_query([item], icl_demonstrs)\n",
    "    \n",
    "    return get_output_for_query(querys[0], im_lists[0],\"llava_v1\",5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e929b2-352b-44f8-8a74-4a83c1aaacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "use_random = False\n",
    "n_random = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b30b149-e125-417a-8a20-36415c01c4a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "out_data = {\n",
    "    \"outputs\": []\n",
    "}\n",
    "\n",
    "model_name = model_name_or_path.split(\"/\")[-1]\n",
    "save_path = f\"/home/asureddy_umass_edu/cs682/VILA/results/vqa_exp/{model_name}_{n}-shot\"\n",
    "if use_random:\n",
    "    save_path += \"_random-examples\"\n",
    "if n_random:\n",
    "    save_path += f\"{n_random}_random-examples\"\n",
    "save_path += \".json\"\n",
    "# print(args)\n",
    "# doing for a max of 10k examples\n",
    "for i in tqdm(range(min(100, len(val_dataset)))):\n",
    "    out = get_output(val_dataset[i],n,use_random, n_random)\n",
    "    out_data[\"outputs\"].append(out)\n",
    "\n",
    "with open(save_path,'w') as f:\n",
    "    json.dump(out_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac802e1-d60e-4673-ba44-02295b5ea81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[2024-11-17 16:27:43,968] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/open_clip/factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 21 files: 100%|██████████████████████| 21/21 [00:00<00:00, 3344.23it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [00:33<00:00,  5.59s/it]\n",
      "Namespace(model_name_or_path='Efficient-Large-Model/VILA1.5-13b', n_shots=8, use_random=False, n_random=0)\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 100/100 [02:17<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "!python vila_e2e_vqa_coco.py --n_shots 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50bcd869-ee13-4626-b3ba-963c6d1e1c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 17 20:19:36 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:C0:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             72W /  400W |    7399MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    700240      C   ...edu/.conda/envs/vila/bin/python3.10       7390MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fefac7e-e6d1-4e73-946a-53c5da9bcf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[2024-11-17 20:07:40,180] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/open_clip/factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 17 files: 100%|██████████████████████| 17/17 [00:00<00:00, 2150.34it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.11s/it]\n",
      "new vqa_coco vila 3b\n",
      "Namespace(model_name_or_path='Efficient-Large-Model/VILA1.5-3b', n_shots=8, use_random=False, n_random=0)\n",
      "100%|█████████████████████████████████████████| 100/100 [01:12<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "!python vila_e2e_vqa_coco.py --n_shots 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007c406c-ddae-4c58-bcb6-7eb7ea2a9b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[2024-11-17 20:10:06,364] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/open_clip/factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "/home/asureddy_umass_edu/.conda/envs/vila/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 17 files: 100%|█████████████████████| 17/17 [00:00<00:00, 18510.69it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.91s/it]\n",
      "Namespace(model_name_or_path='Efficient-Large-Model/VILA1.5-3b', n_shots=8, use_random=False, n_random=0)\n",
      "100%|█████████████████████████████████████████| 100/100 [01:36<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "!python vila_e2e_captioning_coco.py --n_shots 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47cab2-7ffe-4c9a-a799-9820c0761936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
